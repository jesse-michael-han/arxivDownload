{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# Modules\n",
    "import xml.etree.ElementTree as ET\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "import parsing_xml as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exml = ET.parse('tests/latexmled_files/math.0407523.xml')\n",
    "ns = {'latexml': 'http://dlmf.nist.gov/LaTeXML' }\n",
    "para_lst = exml.findall('.//latexml:para', ns)\n",
    "para_text = [px.recutext1(p) for p in para_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in para_text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.023*\"extension\" + 0.023*\"system\" + 0.023*\"coherent\"')\n",
      "(1, '0.022*\"coherent\" + 0.022*\"system\" + 0.018*\"follows\"')\n",
      "(2, '0.034*\"value\" + 0.024*\"suppose\" + 0.023*\"critical\"')\n"
     ]
    }
   ],
   "source": [
    "for l in ldamodel.print_topics(num_topics=3, num_words=3):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title:\n",
    "#### ON THE GEOMETRY OF MODULI SPACES OF COHERENT SYSTEMS ON ALGEBRAIC CURVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the dataset\n",
    "allData = pd.DataFrame()\n",
    "with open('data/out_defs.txt','r') as f1:\n",
    "    all_data_texts = f1.readlines()\n",
    "all_data_labels = len(all_data_texts)*[1.0]\n",
    "with open('data/out_rand.txt', 'r') as f2:\n",
    "    all_data_texts_rand = f2.readlines()\n",
    "all_data_texts += all_data_texts_rand\n",
    "all_data_labels += len(all_data_texts_rand)*[0.0]\n",
    "\n",
    "# 1.0 will represent definitions is true 0.0 means it is false (not a definition)\n",
    "allData['labels'] = all_data_labels\n",
    "allData['texts'] = all_data_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(allData['texts'], allData['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168    there is a homomorphism           which takes ...\n",
       "183    laboratoire j.–a. dieudonné,   université de n...\n",
       "0             une orbifolde pure est un espace analyt...\n",
       "16     the functor             is the covariant funct...\n",
       "75     the functor                                   ...\n",
       "28              [ in the special case that ] has a ge...\n",
       "82            let  be a subscheme of . then a resolut...\n",
       "101    given a vector bundle         on , a sub-bundl...\n",
       "150                                                   \\n\n",
       "237           soit  une surface  lisse, et  un divise...\n",
       "208    in the next paragraph, we just need that      ...\n",
       "177                                                   \\n\n",
       "203    we keep the same notation as in the proof of t...\n",
       "40     fix         and a -graded -module . the euler–...\n",
       "130    by morita equivalence,         is isomorphic t...\n",
       "94     a semi-stable curve           is called stable...\n",
       "115    let           be a semisimple conjugacy class ...\n",
       "163    this work is devoted to the comparisons betwee...\n",
       "49     if         is a reduced divisor on a factorial...\n",
       "122    let             be an exceptional collection. ...\n",
       "44     let         be any -graded -module. a vector  ...\n",
       "108    an ordinary double point         of a projecti...\n",
       "197    since the      -index of a complete fan has no...\n",
       "52              . —  let  be a minuscule weight and l...\n",
       "188    if     is a locally conical divisor along  or ...\n",
       "Name: texts, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', tokenizer=nltk.word_tokenize, ngram_range=(1,2))\n",
    "count_vect.fit(allData['texts'])\n",
    "xtrain = count_vect.transform(train_x)\n",
    "xtest = count_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.61      0.87      0.71        23\n",
      "        1.0       0.90      0.67      0.76        39\n",
      "\n",
      "avg / total       0.79      0.74      0.75        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = naive_bayes.MultinomialNB().fit(xtrain, train_y)\n",
    "predictions = clf.predict(xtest)\n",
    "print(metrics.classification_report(predictions,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  3]\n",
      " [13 26]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(predictions,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article ../2004_ends_with_3/math.0412073/psfrag.xml has few paragraphs\n",
      "article ../2004_ends_with_3/math.0412073/bibliography.xml has few paragraphs\n",
      "article ../2004_ends_with_3/math.0412073/preamble.xml has few paragraphs\n",
      "article ../2004_ends_with_3/math.0412533/gtoutput.xml has few paragraphs\n"
     ]
    }
   ],
   "source": [
    "out_file = open('../out_rand.txt','a')\n",
    "ns = {'latexml': 'http://dlmf.nist.gov/LaTeXML' }\n",
    "for f in glob.glob('../2004_ends_with_3/*/*.xml'):\n",
    "    try:\n",
    "        exml = ET.parse(f)\n",
    "        para_lst_nonrand = exml.findall('.//latexml:para',ns)\n",
    "        para_lst = random.sample(para_lst_nonrand, 2)\n",
    "        for p in para_lst:\n",
    "            out_file.write(px.recutext1(p) + \"\\n\")\n",
    "    except ET.ParseError:\n",
    "        pass\n",
    "    except ValueError:\n",
    "        print('article %s has few paragraphs'%f)\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
