{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "import nltk.data\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import pickle\n",
    "from collections import Iterable\n",
    "\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "import ner\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from unwiki import unwiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The results for the search for definition (currently just Wikipedia)\n",
    "with open('data/wiki_definitions_improved.txt', 'r') as wiki_f:\n",
    "    wiki = wiki_f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u.n', 'mixture', 'sow', 'r.a', 'juniper', 'missions', 'eng', 'wings', 'jie', 'z-1', 'ca', 'u.s', 'j.w', 's^2', 'pl', 'hk', 'cf', 'az', 'e.g', 'al', 'ton', \"'is\", 'vibrations', 'neighbourhood', 'spacewalks', 'dr', 'a<sub>2</sub>', '2Ï€', 'x+2', 'p.h.d', 'e.a', 'etc', 'jr', 'i.e', 'ex', 'ginebra', 'st'}\n"
     ]
    }
   ],
   "source": [
    "# Get data and train the Sentence tokenizer\n",
    "# Uses a standard algorithm (Kiss-Strunk) for unsupervised sentence boundary detection\n",
    "text = ''\n",
    "for i in range(550):\n",
    "    text += unwiki.loads(eval(wiki[i].split('-#-%-')[2]))\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "print(tokenizer._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear classifier  \n",
      "thumb|right|In this case, the solid and empty dots can be correctly classified by any number of linear classifiers. H1 (blue) classifies them correctly, as does H2 (red). H2 could be considered \"better\" in the sense that it is also furthest from both groups.\n",
      "H3 (green) fails to correctly classify the dots.\n",
      "\n",
      "If the input feature vector to the classifier is a real vector _inline_math_, then the output score is\n",
      "\n",
      "_display_math_\n",
      "\n",
      "where _inline_math_ is a real vector of weights and f is a function that converts the dot product of the two vectors into the desired output. (In other words, _inline_math_ is a one-form or linear functional mapping _inline_math_ onto R.) The weight vector _inline_math_ is learned from a set of labeled training samples. Often f is a threshold function, which maps all values of _inline_math_ above a certain threshold to the first class and all other values to the second class; e.g., \n",
      "\n",
      "_display_math_\n",
      "\n",
      "A more complex f might give the probability that an item belongs to a certain class.\n",
      "\n",
      "For a two-class classification problem, one can visualize the operation of a linear classifier as splitting a high-dimensional input space with a hyperplane: all points on one side of the hyperplane are classified as \"yes\", while the others are classified as \"no\".\n",
      "\n",
      "A linear classifier is often used in situations where the speed of classification is an issue, since it is often the fastest classifier, especially when _inline_math_ is sparse. Also, linear classifiers often work very well when the number of dimensions in _inline_math_ is large, as in document classification, where each element in _inline_math_ is typically the number of occurrences of a word in a document (see document-term matrix). In such cases, the classifier should be well-regularized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  \\'[[Image:Svm separating hyperplanes.png|thumb|right|In this case, the solid and empty dots can be correctly classified by any number of linear classifiers. H1 (blue) classifies them correctly, as does H2 (red). H2 could be considered \"better\" in the sense that it is also furthest from both groups.\\\\nH3 (green) fails to correctly classify the dots.]]\\\\n\\\\nIf the input feature vector to the classifier is a [[real number|real]] vector <math>\\\\\\\\vec x</math>, then the output score is\\\\n\\\\n:<math>y = f(\\\\\\\\vec{w}\\\\\\\\cdot\\\\\\\\vec{x}) = f\\\\\\\\left(\\\\\\\\sum_j w_j x_j\\\\\\\\right),</math>\\\\n\\\\nwhere <math>\\\\\\\\vec w</math> is a real vector of weights and \\\\\\'\\\\\\'f\\\\\\'\\\\\\' is a function that converts the [[dot product]] of the two vectors into the desired output. (In other words, <math>\\\\\\\\vec{w}</math> is a [[one-form]] or [[linear functional]] mapping <math>\\\\\\\\vec x</math> onto \\\\\\'\\\\\\'\\\\\\'R\\\\\\'\\\\\\'\\\\\\'.) The weight vector <math>\\\\\\\\vec w</math> is learned from a set of labeled training samples. Often \\\\\\'\\\\\\'f\\\\\\'\\\\\\' is a \\\\\\'\\\\\\'\\\\\\'threshold function\\\\\\'\\\\\\'\\\\\\', which maps all values of <math>\\\\\\\\vec{w}\\\\\\\\cdot\\\\\\\\vec{x}</math> above a certain threshold to the first class and all other values to the second class; e.g., \\\\n\\\\n:<math>\\\\nf(\\\\\\\\mathbf{x}) = \\\\\\\\begin{cases}1 & \\\\\\\\text{if }\\\\\\\\ \\\\\\\\mathbf{w} \\\\\\\\cdot \\\\\\\\mathbf{x} > T,\\\\\\\\\\\\\\\\0 & \\\\\\\\text{otherwise}\\\\\\\\end{cases}\\\\n</math>\\\\n\\\\nA more complex \\\\\\'\\\\\\'f\\\\\\'\\\\\\' might give the probability that an item belongs to a certain class.\\\\n\\\\nFor a two-class classification problem, one can visualize the operation of a linear classifier as splitting a [[High-dimensional space|high-dimensional]] input space with a [[hyperplane]]: all points on one side of the hyperplane are classified as \"yes\", while the others are classified as \"no\".\\\\n\\\\nA linear classifier is often used in situations where the speed of classification is an issue, since it is often the fastest classifier, especially when <math>\\\\\\\\vec x</math> is sparse. Also, linear classifiers often work very well when the number of dimensions in <math>\\\\\\\\vec x</math> is large, as in [[document classification]], where each element in <math>\\\\\\\\vec x</math> is typically the number of occurrences of a word in a document (see [[document-term matrix]]). In such cases, the classifier should be well-[[regularization (machine learning)|regularized]].\\'\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The cleaning up of the wiki markup so that it looks like normal written english\n",
    "title, section, defin = wiki[550].split('-#-%-')\n",
    "dclean = unwiki.loads(eval(defin))\n",
    "print(title)\n",
    "print(dclean)\n",
    "defin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data and POS and NER tags for each definition (LONG TIME)\n",
    "def_lst = []\n",
    "for i in range(len(wiki)):\n",
    "    try:\n",
    "        title, section, defin_raw = wiki[i].split('-#-%-')\n",
    "        defin_all = unwiki.loads(eval(defin_raw))\n",
    "        for d in tokenizer.tokenize(defin_all):\n",
    "            if title.lower().strip() in d.lower():\n",
    "                pos_tokens = pos_tag(word_tokenize(d))\n",
    "                def_ner = ner.bio_tag.bio_tagger(title.strip().split(), pos_tokens)\n",
    "                other_ner = [((d[0],d[1]),d[2]) for d in def_ner]\n",
    "                tmp_dict = {'title': title,\n",
    "                           'section': section,\n",
    "                           'defin': d,\n",
    "                           'ner': other_ner}\n",
    "                def_lst.append(tmp_dict)\n",
    "    except ValueError:\n",
    "        print('parsing error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples = 12709\n",
      "#test samples = 1413\n"
     ]
    }
   ],
   "source": [
    "class NamedEntityChunker(ChunkParserI):\n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    " \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    " \n",
    "        # Transform the result from [((w1, t1), iob1), ...] \n",
    "        # to the preferred list of triplets format [(w1, t1, iob1), ...]\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    " \n",
    "        # Transform the list of triplets to nltk.Tree format\n",
    "        return conlltags2tree(iob_triplets)       \n",
    "        \n",
    "random.shuffle(def_lst)\n",
    "training_samples = [d['ner'] for d in def_lst[:int(len(def_lst) * 0.9)]]\n",
    "test_samples = [d['ner'] for d in def_lst[int(len(def_lst) * 0.9):]]\n",
    " \n",
    "print(\"#training samples = %s\" % len(training_samples) )   # training samples = 55809\n",
    "print(\"#test samples = %s\" % len(test_samples))            # test samples = 6201\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 117 ms, total: 1min 32s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "#train the NER Chunking Classifier (TAKES A LONG TIME)\n",
    "%time chunker = NamedEntityChunker(random.sample(training_samples, len(training_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  89.7%%\n",
      "    Precision:     29.1%%\n",
      "    Recall:        71.5%%\n",
      "    F-Measure:     41.3%%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the most common metrics on the test dataset\n",
    "unpack = lambda l: [(tok, pos, ner) for ((tok, pos), ner) in l]\n",
    "Tree_lst = [conlltags2tree(unpack(t)) for t in test_samples]\n",
    "print(chunker.evaluate(Tree_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  We/PRP\n",
      "  define/VBP\n",
      "  a/DT\n",
      "  (DFNDUM Banach/NNP space/NN)\n",
      "  as/IN\n",
      "  a/DT\n",
      "  (DFNDUM complete/JJ vector/NN space/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# An example of a user fed definition\n",
    "print(chunker.parse(pos_tag(word_tokenize(\"We define a Banach space as a complete vector space.\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_metrics(int_range, chunker_fn, data_set = test_samples, print_output=False):\n",
    "    '''\n",
    "    `int_range` is an integer range\n",
    "    NEEDS A TEST_SAMPLES VARIABLE CREATED WHEN SPLITTING THE \n",
    "    TRAINING AND TESTING DATA\n",
    "    Returns two vectors ready to be used in the \n",
    "    metrics classification function\n",
    "    '''\n",
    "    if isinstance(int_range, int):\n",
    "        int_range = [int_range]\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in int_range:\n",
    "        sample = data_set[i]\n",
    "        sm = [s[0] for s in sample]\n",
    "        y_true_tmp = [s[1] for s in sample]\n",
    "        predicted = [v[2] for v in tree2conlltags(chunker_fn.parse(sm))]\n",
    "        y_true += y_true_tmp\n",
    "        y_pred += predicted\n",
    "        if print_output:\n",
    "            for k,s in enumerate(sm):\n",
    "                print('{:15} {:>10}  {:>10}'.format(s[0], y_true_tmp[k], predicted[k]))\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There                    O           O\n",
      "is                       O           O\n",
      "a                        O           O\n",
      "universal                O    B-DFNDUM\n",
      "Mennicke          B-DFNDUM    I-DFNDUM\n",
      "symbol            I-DFNDUM    I-DFNDUM\n",
      "with                     O           O\n",
      "values                   O           O\n",
      "in                       O           O\n",
      "a                        O           O\n",
      "group                    O           O\n",
      "C                        O           O\n",
      "<                        O           O\n",
      "sub                      O           O\n",
      ">                        O           O\n",
      "q                        O           O\n",
      "<                        O           O\n",
      "/sub                     O           O\n",
      ">                        O           O\n",
      "such                     O           O\n",
      "that                     O           O\n",
      "any                      O           O\n",
      "Mennicke          B-DFNDUM    B-DFNDUM\n",
      "symbol            I-DFNDUM    I-DFNDUM\n",
      "with                     O           O\n",
      "values                   O           O\n",
      "in                       O           O\n",
      "C                        O           O\n",
      "can                      O           O\n",
      "be                       O           O\n",
      "obtained                 O           O\n",
      "by                       O           O\n",
      "composing                O           O\n",
      "the                      O           O\n",
      "universal                O    B-DFNDUM\n",
      "Mennicke          B-DFNDUM    I-DFNDUM\n",
      "symbol            I-DFNDUM    I-DFNDUM\n",
      "with                     O           O\n",
      "a                        O           O\n",
      "unique                   O           O\n",
      "homomorphism             O           O\n",
      "from                     O           O\n",
      "C                        O           O\n",
      "<                        O           O\n",
      "sub                      O           O\n",
      ">                        O           O\n",
      "q                        O           O\n",
      "<                        O           O\n",
      "/sub                     O           O\n",
      ">                        O           O\n",
      "to                       O           O\n",
      "C                        O           O\n",
      ".                        O           O\n"
     ]
    }
   ],
   "source": [
    "OO = prepare_for_metrics(9789, chunker, data_set=training_samples, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    B-DFNDUM       0.33      0.80      0.46      1296\n",
      "    I-DFNDUM       0.26      0.83      0.39      1051\n",
      "           O       0.99      0.90      0.94     46449\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     48796\n",
      "   macro avg       0.52      0.84      0.60     48796\n",
      "weighted avg       0.96      0.90      0.92     48796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, predicted = prepare_for_metrics(range(len(test_samples)), chunker)\n",
    "print(metrics.classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vibrations', 'hk', 'digits', 'pl', 'wings', 'e.g', 'etc', 'x+2', 'ginebra', 'u.s', 'cover', 'i.e', 'dr', 'a<sub>2</sub>', 'missions', 'juniper', 'applications', 'r.a', 'eng', 'contaminants', 'estimated', \"'is\", 'al'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START3]', '[START3]'),('[START2]', '[START2]'), ('[START1]', '[START1]')] +\\\n",
    "    list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]'), ('[END3]', '[END3]')]\n",
    "    history = ['[START3]', '[START2]', '[START1]'] + list(history)\n",
    " \n",
    "    # shift the index with 3, to accommodate the padding\n",
    "    index += 3\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    prev3word, prev3pos = tokens[index - 3]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    next3word, next3pos = tokens[index + 3]\n",
    "    previob = history[index - 1]\n",
    "    prevpreviob = history[index - 2]\n",
    "    prev3iob = history[index - 3]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    " \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    "    \n",
    "    is_math = lambda w:(w == '_inline_math_') or (w == '_display_math_')\n",
    "    ismath = is_math(word)\n",
    "    isprevmath = is_math(prevword)\n",
    "    isprevprevmath = is_math(prevprevword)\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "                'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'next3word': next3word,\n",
    "        'next3pos': next3pos,\n",
    "        \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    " \n",
    "        'prev3word': prev3word,\n",
    "        'prev3pos': prev3pos,\n",
    "        \n",
    "        'prev-iob': previob,\n",
    "        \n",
    "        'prev-prev-iob': prevpreviob,\n",
    " \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot,\n",
    " \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    " \n",
    "        'prev-all-caps': prevallcaps,\n",
    "        'prev-capitalized': prevcapitalized,\n",
    " \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "        \n",
    "        'ismath': ismath,\n",
    "        'isprevmath': isprevmath,\n",
    "        'isprevprevmath': isprevprevmath,\n",
    "    }\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
