{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './unwiki')\n",
    "import unwiki\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "import nltk.data\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import pickle\n",
    "from collections import Iterable\n",
    "\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "import ner\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../wiki_definitions_improved.txt', 'r') as wiki_f:\n",
    "    wiki = wiki_f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astronaut  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' aboard Freedom 7\\n\\nThe criteria for what constitutes human spaceflight vary. The Fédération Aéronautique Internationale (FAI) Sporting Code for astronautics recognizes only flights that exceed an altitude of . In the United States, professional, military, and commercial astronauts who travel above an altitude of  are awarded astronaut wings.\\n\\n, a total of 552 people from 36 countries have reached  or more in altitude, of which 549 reached low Earth orbit or beyond.\\nOf these, 24 people have traveled beyond low Earth orbit, either to lunar orbit, the lunar surface, or, in one case, a loop around the Moon. Three of the 24–Jim Lovell, John Young and Eugene Cernan–did so twice. The three current astronauts who have flown without reaching low Earth orbit are spaceplane pilots Joe Walker, Mike Melvill, and Brian Binnie, who participated in suborbital missions.\\n\\n, under the U.S. definition, 558 people qualify as having reached space, above  altitude. Of eight X-15 pilots who exceeded  in altitude, only one exceeded 100 kilometers (about 62 miles). , the man with the longest cumulative time in space is Gennady Padalka, who has spent 879 days in space. Peggy A. Whitson holds the record for the most time in space by a woman, 377 days.'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title, section, defini = wiki[1].split('-#-%-')\n",
    "print(title)\n",
    "unwiki.loads(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data and train the Sentence tokenizer\n",
    "text = ''\n",
    "for i in range(250):\n",
    "    text += unwiki.loads(eval(wiki[i].split('---')[2]))\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A normed vector space is a pair _inline_math_ where _inline_math_ is a vector space and _inline_math_ a norm on _inline_math_.',\n",
       " 'A seminormed vector space is a pair _inline_math_ where _inline_math_ is a vector space and _inline_math_ a seminorm on _inline_math_.',\n",
       " 'We often omit _inline_math_ or _inline_math_ and just write _inline_math_ for a space if it is clear from the context what (semi) norm we are using.',\n",
       " 'In a more general sense, a vector norm can be taken to be any real-valued function that satisfies the three properties above.',\n",
       " 'A useful variation of the triangle inequality is\\n:_inline_math_ for any vectors x and y.',\n",
       " 'This also shows that a vector norm is a continuous function.',\n",
       " 'Note that property 2 depends on a choice of norm _inline_math_ on the field of scalars.',\n",
       " 'When the scalar field is _inline_math_ (or more generally a subset of _inline_math_), this is usually taken to be the ordinary absolute value, but other choices are possible.',\n",
       " 'For example, for a vector space over _inline_math_ one could take _inline_math_ to be the p-adic norm, which gives rise to a different class of normed vector spaces.']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title, section, defin = wiki[210].split('-#-%-')\n",
    "dclean = unwiki.loads(eval(defin))\n",
    "tokenizer.tokenize(dclean)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_lst = []\n",
    "for i in range(len(wiki)):\n",
    "    try:\n",
    "        title, section, defin_raw = wiki[i].split('-#-%-')\n",
    "        defin_all = unwiki.loads(eval(defin_raw))\n",
    "        for d in tokenizer.tokenize(defin_all):\n",
    "            if title.lower().strip() in d.lower():\n",
    "                pos_tokens = pos_tag(word_tokenize(d))\n",
    "                def_ner = ner.bio_tag.bio_tagger(title.strip().split(), pos_tokens)\n",
    "                other_ner = [((d[0],d[1]),d[2]) for d in def_ner]\n",
    "                tmp_dict = {'title': title,\n",
    "                           'section': section,\n",
    "                           'defin': d,\n",
    "                           'ner': other_ner}\n",
    "                def_lst.append(tmp_dict)\n",
    "    except ValueError:\n",
    "        print('parsing error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('As', 'IN'), 'O'),\n",
       " (('with', 'IN'), 'O'),\n",
       " (('dharma', 'NN'), 'B-DFNDUM'),\n",
       " ((',', ','), 'O'),\n",
       " (('the', 'DT'), 'O'),\n",
       " (('word', 'NN'), 'O'),\n",
       " (('adharma', 'NN'), 'O'),\n",
       " (('includes', 'VBZ'), 'O'),\n",
       " (('and', 'CC'), 'O'),\n",
       " (('implies', 'NNS'), 'O'),\n",
       " (('many', 'JJ'), 'O'),\n",
       " (('ideas', 'NNS'), 'O'),\n",
       " ((';', ':'), 'O'),\n",
       " (('in', 'IN'), 'O'),\n",
       " (('common', 'JJ'), 'O'),\n",
       " (('parlance', 'NN'), 'O'),\n",
       " ((',', ','), 'O'),\n",
       " (('adharma', 'NN'), 'O'),\n",
       " (('means', 'NNS'), 'O'),\n",
       " (('that', 'IN'), 'O'),\n",
       " (('which', 'WDT'), 'O'),\n",
       " (('is', 'VBZ'), 'O'),\n",
       " (('against', 'IN'), 'O'),\n",
       " (('nature', 'NN'), 'O'),\n",
       " ((',', ','), 'O'),\n",
       " (('immoral', 'JJ'), 'O'),\n",
       " ((',', ','), 'O'),\n",
       " (('unethical', 'JJ'), 'O'),\n",
       " ((',', ','), 'O'),\n",
       " (('wrong', 'JJ'), 'O'),\n",
       " (('or', 'CC'), 'O'),\n",
       " (('unlawful', 'JJ'), 'O'),\n",
       " (('.', '.'), 'O')]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = def_lst[414]['ner']\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples = 12056\n",
      "#test samples = 1340\n"
     ]
    }
   ],
   "source": [
    "class NamedEntityChunker(ChunkParserI):\n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    " \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    " \n",
    "        # Transform the result from [((w1, t1), iob1), ...] \n",
    "        # to the preferred list of triplets format [(w1, t1, iob1), ...]\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    " \n",
    "        # Transform the list of triplets to nltk.Tree format\n",
    "        return conlltags2tree(iob_triplets)       \n",
    "        \n",
    "training_samples = [d['ner'] for d in def_lst[:int(len(def_lst) * 0.9)]]\n",
    "test_samples = [d['ner'] for d in def_lst[int(len(def_lst) * 0.9):]]\n",
    " \n",
    "print(\"#training samples = %s\" % len(training_samples) )   # training samples = 55809\n",
    "print(\"#test samples = %s\" % len(test_samples))            # test samples = 6201\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 1.11 s, total: 1min 49s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "#train takes a long time\n",
    "%time chunker = NamedEntityChunker(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  We/PRP\n",
      "  define/VBP\n",
      "  a/DT\n",
      "  (DFNDUM Banach/NNP space/NN)\n",
      "  as/IN\n",
      "  a/DT\n",
      "  (DFNDUM complete/JJ vector/NN space/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(chunker.parse(pos_tag(word_tokenize(\"We define a Banach space as a complete vector space.\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-c5c33fba4c91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not range"
     ]
    }
   ],
   "source": [
    "test_samples[range(2,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_metrics(int_range):\n",
    "    '''\n",
    "    `int_range` is an integer range\n",
    "    NEEDS A TEST_SAMPLES VARIABLE CREATED WHEN SPLITTING THE \n",
    "    TRAINING AND TESTING DATA\n",
    "    Returns two vectors ready to be used in the \n",
    "    metrics classification function\n",
    "    '''\n",
    "    if isinstance(int_range, int):\n",
    "        int_range = [int_range]\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in int_range:\n",
    "        sample = test_samples[i]\n",
    "        sm = [s[0] for s in sample]\n",
    "        y_true_tmp = [s[1] for s in sample]\n",
    "        predicted = [v[2] for v in tree2conlltags(chunker.parse(sm))]\n",
    "        y_true += y_true_tmp\n",
    "        y_pred += predicted\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    B-DFNDUM       0.30      0.73      0.43      1260\n",
      "    I-DFNDUM       0.29      0.75      0.42      1232\n",
      "           O       0.99      0.90      0.94     43929\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     46421\n",
      "   macro avg       0.52      0.80      0.59     46421\n",
      "weighted avg       0.95      0.89      0.91     46421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, predicted = prepare_for_metrics(range(len(test_samples)))\n",
    "print(metrics.classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(45,int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'etc', 'dr', 'al', 'ice', 'pl', 'u.s', 'hz', 'i.e', 'uk', 'e.g', 'rna'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START2]', '[START2]'), ('[START1]', '[START1]')] +\\\n",
    "    list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]')]\n",
    "    history = ['[START2]', '[START1]'] + list(history)\n",
    " \n",
    "    # shift the index with 2, to accommodate the padding\n",
    "    index += 2\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    previob = history[index - 1]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    " \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "                'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    " \n",
    "        'prev-iob': previob,\n",
    " \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot,\n",
    " \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    " \n",
    "        'prev-all-caps': prevallcaps,\n",
    "        'prev-capitalized': prevcapitalized,\n",
    " \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "    }\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
