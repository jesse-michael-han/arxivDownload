{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "import nltk.data\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import pickle\n",
    "from collections import Iterable\n",
    "\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "import ner\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from unwiki import unwiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../wiki_definitions_improved.txt', 'r') as wiki_f:\n",
    "    wiki = wiki_f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data and train the Sentence tokenizer\n",
    "text = ''\n",
    "for i in range(550):\n",
    "    text += unwiki.loads(eval(wiki[i].split('-#-%-')[2]))\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predation  \n",
      "\n",
      "\n",
      "At the most basic level, predators kill and eat other organisms. However, the concept of predation is broad, defined differently in different contexts, and includes a wide variety of feeding methods; and some relationships that result in the prey's death are not generally called predation. A parasitoid, such as an ichneumon wasp, lays its eggs in or on its host; the eggs hatch into larvae, which eat the host, and it inevitably dies. Zoologists generally call this a form of parasitism, though conventionally parasites are thought not to kill their hosts. A predator can be defined to differ from a parasitoid in two ways: it kills its prey immediately; and it has many prey, captured over its lifetime, where a parasitoid's larva has just one, or at least has its food supply provisioned for it on just one occasion. \n",
      "\n",
      "\n",
      "\n",
      "There are other difficult and borderline cases. Micropredators are small animals that, like predators, feed entirely on other organisms; they include fleas and mosquitoes that consume blood from living animals, and aphids that consume sap from living plants. However, since they typically do not kill their hosts, they are now often thought of as parasites. Animals that graze on phytoplankton or mats of microbes are predators, as they consume and kill their food organisms; but herbivores that browse leaves are not, as their food plants usually survive the assault. However, when animals eat seeds (seed predation or granivory) or eggs (egg predation), they are consuming entire living organisms, which by definition makes them predators, albeit unconventional ones: for instance, a mouse that eats grass seeds has no adaptations for tracking, catching and subduing prey and its teeth are not adapted to slicing through flesh.\n",
      "\n",
      "Scavengers, organisms that only eat organisms found already dead, are not predators, but many predators such as the jackal and the hyena scavenge when the opportunity arises. Among invertebrates, social wasps (yellowjackets) are both hunters and scavengers of other insects.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n\\nAt the most basic level, predators kill and eat other organisms.',\n",
       " \"However, the concept of predation is broad, defined differently in different contexts, and includes a wide variety of feeding methods; and some relationships that result in the prey's death are not generally called predation.\",\n",
       " 'A parasitoid, such as an ichneumon wasp, lays its eggs in or on its host; the eggs hatch into larvae, which eat the host, and it inevitably dies.',\n",
       " 'Zoologists generally call this a form of parasitism, though conventionally parasites are thought not to kill their hosts.',\n",
       " \"A predator can be defined to differ from a parasitoid in two ways: it kills its prey immediately; and it has many prey, captured over its lifetime, where a parasitoid's larva has just one, or at least has its food supply provisioned for it on just one occasion.\",\n",
       " 'There are other difficult and borderline cases.',\n",
       " 'Micropredators are small animals that, like predators, feed entirely on other organisms; they include fleas and mosquitoes that consume blood from living animals, and aphids that consume sap from living plants.',\n",
       " 'However, since they typically do not kill their hosts, they are now often thought of as parasites.',\n",
       " 'Animals that graze on phytoplankton or mats of microbes are predators, as they consume and kill their food organisms; but herbivores that browse leaves are not, as their food plants usually survive the assault.',\n",
       " 'However, when animals eat seeds (seed predation or granivory) or eggs (egg predation), they are consuming entire living organisms, which by definition makes them predators, albeit unconventional ones: for instance, a mouse that eats grass seeds has no adaptations for tracking, catching and subduing prey and its teeth are not adapted to slicing through flesh.',\n",
       " 'Scavengers, organisms that only eat organisms found already dead, are not predators, but many predators such as the jackal and the hyena scavenge when the opportunity arises.',\n",
       " 'Among invertebrates, social wasps (yellowjackets) are both hunters and scavengers of other insects.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title, section, defin = wiki[450].split('-#-%-')\n",
    "dclean = unwiki.loads(eval(defin))\n",
    "print(title)\n",
    "print(dclean)\n",
    "tokenizer.tokenize(dclean)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data and POS and NER tags for each definition (LONG TIME)\n",
    "def_lst = []\n",
    "for i in range(len(wiki)):\n",
    "    try:\n",
    "        title, section, defin_raw = wiki[i].split('-#-%-')\n",
    "        defin_all = unwiki.loads(eval(defin_raw))\n",
    "        for d in tokenizer.tokenize(defin_all):\n",
    "            if title.lower().strip() in d.lower():\n",
    "                pos_tokens = pos_tag(word_tokenize(d))\n",
    "                def_ner = ner.bio_tag.bio_tagger(title.strip().split(), pos_tokens)\n",
    "                other_ner = [((d[0],d[1]),d[2]) for d in def_ner]\n",
    "                tmp_dict = {'title': title,\n",
    "                           'section': section,\n",
    "                           'defin': d,\n",
    "                           'ner': other_ner}\n",
    "                def_lst.append(tmp_dict)\n",
    "    except ValueError:\n",
    "        print('parsing error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples = 12649\n",
      "#test samples = 1406\n"
     ]
    }
   ],
   "source": [
    "class NamedEntityChunker(ChunkParserI):\n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    " \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    " \n",
    "        # Transform the result from [((w1, t1), iob1), ...] \n",
    "        # to the preferred list of triplets format [(w1, t1, iob1), ...]\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    " \n",
    "        # Transform the list of triplets to nltk.Tree format\n",
    "        return conlltags2tree(iob_triplets)       \n",
    "        \n",
    "random.shuffle(def_lst)\n",
    "training_samples = [d['ner'] for d in def_lst[:int(len(def_lst) * 0.9)]]\n",
    "test_samples = [d['ner'] for d in def_lst[int(len(def_lst) * 0.9):]]\n",
    " \n",
    "print(\"#training samples = %s\" % len(training_samples) )   # training samples = 55809\n",
    "print(\"#test samples = %s\" % len(test_samples))            # test samples = 6201\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 12s, sys: 58.3 ms, total: 2min 12s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "#train the NER Chunking Classifier (TAKES A LONG TIME)\n",
    "%time chunker = NamedEntityChunker(random.sample(training_samples, len(training_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  We/PRP\n",
      "  define/VBP\n",
      "  a/DT\n",
      "  (DFNDUM Banach/NNP space/NN)\n",
      "  as/IN\n",
      "  a/DT\n",
      "  complete/JJ\n",
      "  vector/NN\n",
      "  space/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# An example of a user fed definition\n",
    "print(chunker.parse(pos_tag(word_tokenize(\"We define a Banach space as a complete vector space.\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_metrics(int_range, chunker_fn, data_set = test_samples, print_output=False):\n",
    "    '''\n",
    "    `int_range` is an integer range\n",
    "    NEEDS A TEST_SAMPLES VARIABLE CREATED WHEN SPLITTING THE \n",
    "    TRAINING AND TESTING DATA\n",
    "    Returns two vectors ready to be used in the \n",
    "    metrics classification function\n",
    "    '''\n",
    "    if isinstance(int_range, int):\n",
    "        int_range = [int_range]\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in int_range:\n",
    "        sample = data_set[i]\n",
    "        sm = [s[0] for s in sample]\n",
    "        y_true_tmp = [s[1] for s in sample]\n",
    "        predicted = [v[2] for v in tree2conlltags(chunker_fn.parse(sm))]\n",
    "        y_true += y_true_tmp\n",
    "        y_pred += predicted\n",
    "        if print_output:\n",
    "            for k,s in enumerate(sm):\n",
    "                print('{:15} {:>10}  {:>10}'.format(s[0], y_true_tmp[k], predicted[k]))\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A                        O           O\n",
      "bit               B-DFNDUM    B-DFNDUM\n",
      "array             I-DFNDUM    I-DFNDUM\n",
      "is                       O           O\n",
      "a                        O           O\n",
      "mapping                  O           O\n",
      "from                     O           O\n",
      "some                     O           O\n",
      "domain                   O           O\n",
      "(                        O           O\n",
      "almost                   O           O\n",
      "always                   O           O\n",
      "a                        O           O\n",
      "range                    O           O\n",
      "of                       O           O\n",
      "integers                 O           O\n",
      ")                        O           O\n",
      "to                       O           O\n",
      "values                   O           O\n",
      "in                       O           O\n",
      "the                      O           O\n",
      "set                      O           O\n",
      "{                        O           O\n",
      "0                        O           O\n",
      ",                        O           O\n",
      "1                        O           O\n",
      "}                        O           O\n",
      ".                        O           O\n"
     ]
    }
   ],
   "source": [
    "OO = prepare_for_metrics(11599, chunker, data_set=training_samples, print_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    B-DFNDUM       0.35      0.77      0.48      1317\n",
      "    I-DFNDUM       0.32      0.80      0.46      1078\n",
      "           O       0.99      0.92      0.96     46866\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     49261\n",
      "   macro avg       0.55      0.83      0.63     49261\n",
      "weighted avg       0.96      0.92      0.93     49261\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, predicted = prepare_for_metrics(range(len(test_samples)), chunker)\n",
    "print(metrics.classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    B-DFNDUM       0.33      0.75      0.46      1337\n",
      "    I-DFNDUM       0.30      0.78      0.43      1110\n",
      "           O       0.99      0.91      0.95     45594\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     48041\n",
      "   macro avg       0.54      0.81      0.62     48041\n",
      "weighted avg       0.95      0.91      0.92     48041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, predicted = prepare_for_metrics(range(len(test_samples)), chunker)\n",
    "print(metrics.classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    B-DFNDUM       0.30      0.73      0.43      1263\n",
      "    I-DFNDUM       0.28      0.75      0.41      1233\n",
      "           O       0.99      0.90      0.94     44197\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     46693\n",
      "   macro avg       0.52      0.80      0.59     46693\n",
      "weighted avg       0.95      0.89      0.91     46693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, predicted = prepare_for_metrics(range(len(test_samples)), chunker9K)\n",
    "print(metrics.classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vibrations', 'hk', 'digits', 'pl', 'wings', 'e.g', 'etc', 'x+2', 'ginebra', 'u.s', 'cover', 'i.e', 'dr', 'a<sub>2</sub>', 'missions', 'juniper', 'applications', 'r.a', 'eng', 'contaminants', 'estimated', \"'is\", 'al'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer._params.abbrev_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START3]', '[START3]'),('[START2]', '[START2]'), ('[START1]', '[START1]')] +\\\n",
    "    list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]'), ('[END3]', '[END3]')]\n",
    "    history = ['[START3]', '[START2]', '[START1]'] + list(history)\n",
    " \n",
    "    # shift the index with 3, to accommodate the padding\n",
    "    index += 3\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    prev3word, prev3pos = tokens[index - 3]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    next3word, next3pos = tokens[index + 3]\n",
    "    previob = history[index - 1]\n",
    "    prevpreviob = history[index - 2]\n",
    "    prev3iob = history[index - 3]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    " \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    "    \n",
    "    is_math = lambda w:(w == '_inline_math_') or (w == '_display_math_')\n",
    "    ismath = is_math(word)\n",
    "    isprevmath = is_math(prevword)\n",
    "    isprevprevmath = is_math(prevprevword)\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "                'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'next3word': next3word,\n",
    "        'next3pos': next3pos,\n",
    "        \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    " \n",
    "        'prev3word': prev3word,\n",
    "        'prev3pos': prev3pos,\n",
    "        \n",
    "        'prev-iob': previob,\n",
    "        \n",
    "        'prev-prev-iob': prevpreviob,\n",
    " \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot,\n",
    " \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    " \n",
    "        'prev-all-caps': prevallcaps,\n",
    "        'prev-capitalized': prevcapitalized,\n",
    " \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "        \n",
    "        'ismath': ismath,\n",
    "        'isprevmath': isprevmath,\n",
    "        'isprevprevmath': isprevprevmath,\n",
    "    }\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
