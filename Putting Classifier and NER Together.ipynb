{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import sys\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "import nltk.data\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import pickle\n",
    "from collections import Iterable\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Local imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from unwiki import unwiki\n",
    "import ner\n",
    "import parsing_xml as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.91      0.81      2217\n",
      "         1.0       0.95      0.84      0.89      4661\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6878\n",
      "   macro avg       0.84      0.87      0.85      6878\n",
      "weighted avg       0.88      0.86      0.87      6878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define Clean function to cleanse and standarize words\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalize\n",
    "\n",
    "#prepare the dataset\n",
    "allData = pd.DataFrame()\n",
    "with open('../sample18/defs.txt','r') as f1:\n",
    "    all_data_texts = f1.readlines()\n",
    "all_data_labels = len(all_data_texts)*[1.0]\n",
    "with open('../sample18/nondefs.txt', 'r') as f2:\n",
    "    all_data_texts_rand = f2.readlines()\n",
    "all_data_texts += all_data_texts_rand\n",
    "all_data_labels += len(all_data_texts_rand)*[0.0]\n",
    "\n",
    "# 1.0 will represent definitions is true 0.0 means it is false (not a definition)\n",
    "allData['labels'] = all_data_labels\n",
    "allData['texts'] = all_data_texts\n",
    "\n",
    "# Split and randomize the datasets\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(allData['texts'], allData['labels'])\n",
    "\n",
    "# Vectorize all the paragraphs and definitions in the dataset\n",
    "count_vect = CountVectorizer(analyzer='word', tokenizer=nltk.word_tokenize, ngram_range=(1,2))\n",
    "count_vect.fit(allData['texts'])\n",
    "xtrain = count_vect.transform(train_x)\n",
    "xtest = count_vect.transform(test_x)\n",
    "\n",
    "# Train Multinomial Naive Bayes model and print test metrics\n",
    "clf = naive_bayes.MultinomialNB().fit(xtrain, train_y)\n",
    "predictions = clf.predict(xtest)\n",
    "print(metrics.classification_report(predictions,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2b250b3d3ce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m       \u001b[0;34m'There is no real reason as to why this classifier is so good.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       'a triangle is equilateral if and only if all its sides are the same length.']\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mvdef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvdef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count_vect' is not defined"
     ]
    }
   ],
   "source": [
    "Def = ['a banach space is defined as a complete vector space.',\n",
    "       'this is not a definition honestly. even if it includes technical words like scheme and cohomology',\n",
    "      'There is no real reason as to why this classifier is so good.',\n",
    "      'a triangle is equilateral if and only if all its sides are the same length.']\n",
    "vdef = count_vect.transform(Def)\n",
    "clf.predict(vdef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples = 12602\n",
      "#test samples = 1401\n",
      "CPU times: user 1min 53s, sys: 650 ms, total: 1min 54s\n",
      "Wall time: 1min 54s\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  91.2%%\n",
      "    Precision:     30.9%%\n",
      "    Recall:        66.2%%\n",
      "    F-Measure:     42.1%%\n",
      "The                      O           O\n",
      "base                     O    B-DFNDUM\n",
      "of                       O           O\n",
      "the                      O           O\n",
      "Copernican        B-DFNDUM    B-DFNDUM\n",
      "period            I-DFNDUM    I-DFNDUM\n",
      "is                       O           O\n",
      "defined                  O           O\n",
      "based                    O           O\n",
      "on                       O           O\n",
      "the                      O           O\n",
      "recognition              O           O\n",
      "that                     O           O\n",
      "freshly                  O           O\n",
      "excavated                O           O\n",
      "materials                O           O\n",
      "on                       O           O\n",
      "the                      O           O\n",
      "lunar                    O    B-DFNDUM\n",
      "surface                  O    I-DFNDUM\n",
      "are                      O           O\n",
      "generally                O           O\n",
      "``                       O           O\n",
      "bright                   O    B-DFNDUM\n",
      "''                       O           O\n",
      "and                      O           O\n",
      "that                     O           O\n",
      "they                     O           O\n",
      "become                   O           O\n",
      "darker                   O           O\n",
      "over                     O           O\n",
      "time                     O           O\n",
      "as                       O           O\n",
      "a                        O           O\n",
      "result                   O           O\n",
      "of                       O           O\n",
      "space                    O           O\n",
      "weathering               O           O\n",
      "processes                O           O\n",
      ".                        O           O\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    B-DFNDUM       0.35      0.75      0.48      1325\n",
      "    I-DFNDUM       0.30      0.82      0.44       997\n",
      "           O       0.99      0.92      0.95     44978\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     47300\n",
      "   macro avg       0.55      0.83      0.62     47300\n",
      "weighted avg       0.96      0.91      0.93     47300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The results for the search for definition (currently just Wikipedia)\n",
    "with open('data/wiki_definitions_improved.txt', 'r') as wiki_f:\n",
    "    wiki = wiki_f.readlines()\n",
    "    \n",
    "# Get data and train the Sentence tokenizer\n",
    "# Splits the individual sentences of a paragraph apart \n",
    "# Uses a standard algorithm (Kiss-Strunk) for unsupervised sentence boundary detection\n",
    "text = ''\n",
    "for i in range(550):\n",
    "    text += unwiki.loads(eval(wiki[i].split('-#-%-')[2]))\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "#print(tokenizer._params.abbrev_types)\n",
    "\n",
    "\n",
    "# Define the accesory function for preparing the feature of the classifier\n",
    "def features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START3]', '[START3]'),('[START2]', '[START2]'), ('[START1]', '[START1]')] +\\\n",
    "    list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]'), ('[END3]', '[END3]')]\n",
    "    history = ['[START3]', '[START2]', '[START1]'] + list(history)\n",
    " \n",
    "    # shift the index with 3, to accommodate the padding\n",
    "    index += 3\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    prev3word, prev3pos = tokens[index - 3]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    next3word, next3pos = tokens[index + 3]\n",
    "    previob = history[index - 1]\n",
    "    prevpreviob = history[index - 2]\n",
    "    prev3iob = history[index - 3]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    " \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    "    \n",
    "    is_math = lambda w:(w == '_inline_math_') or (w == '_display_math_')\n",
    "    ismath = is_math(word)\n",
    "    isprevmath = is_math(prevword)\n",
    "    isprevprevmath = is_math(prevprevword)\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "                'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'next3word': next3word,\n",
    "        'next3pos': next3pos,\n",
    "        \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    " \n",
    "        'prev3word': prev3word,\n",
    "        'prev3pos': prev3pos,\n",
    "        \n",
    "        'prev-iob': previob,\n",
    "        \n",
    "        'prev-prev-iob': prevpreviob,\n",
    " \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot,\n",
    " \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    " \n",
    "        'prev-all-caps': prevallcaps,\n",
    "        'prev-capitalized': prevcapitalized,\n",
    " \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "        \n",
    "        'ismath': ismath,\n",
    "        'isprevmath': isprevmath,\n",
    "        'isprevprevmath': isprevprevmath,\n",
    "    }\n",
    "\n",
    "# Get the data and POS and NER tags for each definition (LONG TIME)\n",
    "def_lst = []\n",
    "for i in range(len(wiki)):\n",
    "    try:\n",
    "        title, section, defin_raw = wiki[i].split('-#-%-')\n",
    "        defin_all = unwiki.loads(eval(defin_raw))\n",
    "        for d in tokenizer.tokenize(defin_all):\n",
    "            if title.lower().strip() in d.lower():\n",
    "                pos_tokens = pos_tag(word_tokenize(d))\n",
    "                def_ner = ner.bio_tag.bio_tagger(title.strip().split(), pos_tokens)\n",
    "                other_ner = [((d[0],d[1]),d[2]) for d in def_ner]\n",
    "                tmp_dict = {'title': title,\n",
    "                           'section': section,\n",
    "                           'defin': d,\n",
    "                           'ner': other_ner}\n",
    "                def_lst.append(tmp_dict)\n",
    "    except ValueError:\n",
    "        print('parsing error')\n",
    "        \n",
    "# The ChunkParserI has to be instantiated        \n",
    "class NamedEntityChunker(ChunkParserI):\n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    " \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    " \n",
    "        # Transform the result from [((w1, t1), iob1), ...] \n",
    "        # to the preferred list of triplets format [(w1, t1, iob1), ...]\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    " \n",
    "        # Transform the list of triplets to nltk.Tree format\n",
    "        return conlltags2tree(iob_triplets)       \n",
    "        \n",
    "random.shuffle(def_lst)\n",
    "training_samples = [d['ner'] for d in def_lst[:int(len(def_lst) * 0.9)]]\n",
    "test_samples = [d['ner'] for d in def_lst[int(len(def_lst) * 0.9):]]\n",
    " \n",
    "print(\"#training samples = %s\" % len(training_samples) )   \n",
    "print(\"#test samples = %s\" % len(test_samples))            \n",
    "\n",
    "#train the NER Chunking Classifier \n",
    "%time chunker = NamedEntityChunker(training_samples)\n",
    "\n",
    "# Evaluate the most common metrics on the test dataset\n",
    "unpack = lambda l: [(tok, pos, ner) for ((tok, pos), ner) in l]\n",
    "Tree_lst = [conlltags2tree(unpack(t)) for t in test_samples]\n",
    "print(chunker.evaluate(Tree_lst))\n",
    "\n",
    "\n",
    "def prepare_for_metrics(int_range, chunker_fn, data_set = test_samples, print_output=False):\n",
    "    '''\n",
    "    Accesory function for computing metrics\n",
    "    `int_range` is an integer range\n",
    "    NEEDS A TEST_SAMPLES VARIABLE CREATED WHEN SPLITTING THE \n",
    "    TRAINING AND TESTING DATA\n",
    "    Returns two vectors ready to be used in the \n",
    "    metrics classification function\n",
    "    '''\n",
    "    if isinstance(int_range, int):\n",
    "        int_range = [int_range]\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for i in int_range:\n",
    "        sample = data_set[i]\n",
    "        sm = [s[0] for s in sample]\n",
    "        y_true_tmp = [s[1] for s in sample]\n",
    "        predicted = [v[2] for v in tree2conlltags(chunker_fn.parse(sm))]\n",
    "        y_true += y_true_tmp\n",
    "        y_pred += predicted\n",
    "        if print_output:\n",
    "            for k,s in enumerate(sm):\n",
    "                print('{:15} {:>10}  {:>10}'.format(s[0], y_true_tmp[k], predicted[k]))\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Prepare and print metrics for the normal metrics\n",
    "OO = prepare_for_metrics(119, chunker, data_set=test_samples, print_output=True)\n",
    "y_true, predicted = prepare_for_metrics(range(len(test_samples)), chunker)\n",
    "print(metrics.classification_report(y_true, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'banach space'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of a user fed definition\n",
    "chunked = chunker.parse(pos_tag(word_tokenize(Def[0])))\n",
    "D =list(filter(lambda x: isinstance(x, nltk.tree.Tree), chunked))[0]\n",
    "' '.join([d[0] for d in D])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "art = px.DefinitionsXML('tests/latexmled_files/1501.06563.xml')\n",
    "p_lst = [px.recutext_xml(p) for p in art.tag_list(tag='para')] \n",
    "p_vec = count_vect.transform(p_lst)\n",
    "preds = clf.predict(p_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0  In 1990 Lazard _citation_ proposed an improved projection operation for cylindrical algebraic decom\n",
      "------\n",
      "1 0.0  In _citation_ we study Lazard’s projection. It is shown there that Lazard’s projection is valid for\n",
      "------\n",
      "2 0.0  In this report we study separately a more limited but rigorous concept of Lazard’s valuation: namel\n",
      "------\n",
      "3 0.0  In this section we study Lazard’s valuation _citation_ in a relatively special setting, namely, tha\n",
      "------\n",
      "4 0.0  We first define the Lazard valuation in a limited way. This will allow us to provide simple, straig\n",
      "------\n",
      "5 1.0  We recall at the outset the standard algebraic definition of the term valuation _citation_. A mappi\n",
      "------\n",
      "6 1.0  1. _inline_math_ for all _inline_math_ and _inline_math_; 2. _inline_math_, for all _inline_math_ a\n",
      "------\n",
      "7 1.0  _inline_math_ for all _inline_math_ and _inline_math_; \n",
      "------\n",
      "8 1.0  _inline_math_, for all _inline_math_ and _inline_math_ (with _inline_math_). \n",
      "------\n",
      "9 1.0  By the same axioms one could define the notion of valuation of a ring _citation_. (In such a case _\n",
      "------\n",
      "10 1.0  Let _inline_math_. Recall that the lexicographic order _inline_math_ on _inline_math_ is defined by\n",
      "------\n",
      "11 1.0  Let _inline_math_ be a field. Let _inline_math_ be a nonzero element of the polynomial ring _inline\n",
      "------\n",
      "12 1.0  We remark that _inline_math_ could be defined equivalently to be the element _inline_math_ of _inli\n",
      "------\n",
      "13 1.0  Let _inline_math_. Then _inline_math_ is the familiar order _inline_math_ of _inline_math_ at _inli\n",
      "------\n",
      "14 0.0  Where there is no ambiguity we shall usually omit the qualifier “Lazard” from “Lazard valuation”. W\n",
      "------\n",
      "15 1.0  Let _inline_math_ and _inline_math_ be nonzero elements of _inline_math_ and let _inline_math_. The\n",
      "------\n",
      "16 0.0  These claims follow since _inline_math_, together with componentwise addition and _inline_math_, fo\n",
      "------\n",
      "17 0.0  For the remaining properties we state we shall assume that _inline_math_ or _inline_math_. \n",
      "------\n",
      "18 1.0  (Upper semicontinuity of valuation) Let _inline_math_ be a nonzero element of _inline_math_ and let\n",
      "------\n",
      "19 1.0  Let _inline_math_. By definition of _inline_math_, _inline_math_. By continuity of the function _in\n",
      "------\n",
      "20 1.0  Let _inline_math_. We shall say that _inline_math_ is valuation-invariant in a subset _inline_math_\n",
      "------\n",
      "21 1.0  Let _inline_math_ and _inline_math_ be nonzero elements of _inline_math_ and let _inline_math_ be c\n",
      "------\n",
      "22 0.0  This proposition is analogous to Lemma A.3 of _citation_. Its proof, using Propositions 2.3 and 2.4\n",
      "------\n",
      "23 0.0  As noted previously, the above properties are analogues of those of the familiar order _inline_math\n",
      "------\n",
      "24 1.0  Let _inline_math_ be primitive of positive degree in _inline_math_ and squarefree. Then for all but\n",
      "------\n",
      "25 0.0  Denote by _inline_math_ the resultant _inline_math_ of _inline_math_ and _inline_math_ with respect\n",
      "------\n",
      "26 0.0  Let us consider the relationship between the concepts of order-invariance and valuation-invariance \n",
      "------\n",
      "27 1.0  Let _inline_math_ be nonzero and _inline_math_ be connected. If _inline_math_ is valuation-invarian\n",
      "------\n",
      "28 0.0  Assume that _inline_math_ is valuation-invariant in _inline_math_. Write _inline_math_ as a product\n",
      "------\n",
      "29 0.0  In Section 3 we shall provide an example indicating that Proposition 2.7 is not true for dimension \n",
      "------\n",
      "30 1.0  Let _inline_math_ be a set of elements of _inline_math_. Recall that an _inline_math_-invariant CAD\n",
      "------\n",
      "31 0.0  In this section we first prove a result which implies, roughly speaking, that many of the cells pro\n",
      "------\n",
      "32 0.0  Recall the fundamental concept of delineability reviewed in _citation_. Delineability ensures the c\n",
      "------\n",
      "33 1.0  Let _inline_math_ and let _inline_math_ be a connected subset of _inline_math_. Suppose that _inlin\n",
      "------\n",
      "34 0.0  Let _inline_math_ be a real root function of _inline_math_ on _inline_math_ such that _inline_math_\n",
      "------\n",
      "35 0.0  An element _inline_math_ is nullified by a subset _inline_math_ of _inline_math_ if _inline_math_ f\n",
      "------\n",
      "36 1.0  Let _inline_math_ be a field. (In this section _inline_math_ or, when explicit computation is requi\n",
      "------\n",
      "37 1.0  _inline_math_ \n",
      "------\n",
      "38 1.0  For _inline_math_ to _inline_math_ do _inline_math_ the greatest integer _inline_math_ such that _i\n",
      "------\n",
      "39 1.0  _inline_math_ the greatest integer _inline_math_ such that _inline_math_ \n",
      "------\n",
      "40 1.0  _inline_math_ \n",
      "------\n",
      "41 1.0  _inline_math_ \n",
      "------\n",
      "42 1.0  With _inline_math_, _inline_math_, _inline_math_, _inline_math_ and the _inline_math_ as in the abo\n",
      "------\n",
      "43 0.0  One more definition is needed before we can state Lazard’s main claim: \n",
      "------\n",
      "44 1.0  Let _inline_math_ be nonzero and _inline_math_ a subset of _inline_math_. We say that _inline_math_\n",
      "------\n",
      "45 1.0  the Lazard valuation of _inline_math_ on _inline_math_ is the same for each point _inline_math_; \n",
      "------\n",
      "46 1.0  there exist finitely many continuous functions _inline_math_ from _inline_math_ to _inline_math_, w\n",
      "------\n",
      "47 1.0  there exist positive integers _inline_math_ such that, for all _inline_math_ and all _inline_math_,\n",
      "------\n",
      "48 1.0  Let _inline_math_ and _inline_math_ be as in the above definition of Lazard delineability. Suppose \n",
      "------\n",
      "49 1.0  For a finite irreducible basis in _inline_math_, where _inline_math_, recall that the Lazard projec\n",
      "------\n",
      "50 1.0  Let _inline_math_ be a finite irreducible basis in _inline_math_, where _inline_math_. Let _inline_\n",
      "------\n",
      "51 0.0  This claim concerns valuation-invariant lifting in relation to _inline_math_: it asserts that the c\n",
      "------\n",
      "52 0.0  Suppose _inline_math_ and _inline_math_ is a submanifold of _inline_math_. Then Lazard’s main claim\n",
      "------\n",
      "53 0.0  Suppose first that _inline_math_. By remarks in Example 2.2, the hypothesis implies that each eleme\n",
      "------\n",
      "54 0.0  Suppose second that _inline_math_. The conclusions are essentially trivial in case the dimension of\n",
      "------\n",
      "55 0.0  Now we present an example showing that valuation-invariance does not imply order-invariance when _i\n",
      "------\n",
      "56 0.0  Let _inline_math_ be _inline_math_ or _inline_math_. In this subsection we discuss some of the prob\n",
      "------\n",
      "57 0.0  Now let _inline_math_. We consider the formal power series ring _display_math_ We could define the \n",
      "------\n",
      "58 0.0  Next we consider extension of the valuations _inline_math_ and _inline_math_ to the field of ration\n",
      "------\n",
      "59 1.0  Let _inline_math_ be a nonzero element of _inline_math_, let _inline_math_ be an open set throughou\n",
      "------\n",
      "60 0.0  A direct proof (adapting that of Proposition 2.4) could easily be given. In fact this is a special \n",
      "------\n",
      "61 0.0  Similar remarks and an analogue of the above proposition apply to the Lazard valuation. \n",
      "------\n",
      "62 0.0  Now in case _inline_math_ there is an interesting relationship between the rational function field \n",
      "------\n",
      "63 0.0  Finally we consider possible extension of the valuations and the associated basic theory to fractio\n",
      "------\n",
      "64 0.0  Mindful of the potential benefits of Lazard’s approach to projection in _citation_, yet conscious o\n",
      "------\n",
      "65 0.0  Further work could usefully be done in a number of directions. It would be desirable to extend the \n",
      "------\n",
      "66 0.0  We would like to acknowledge a grant from the Korea Institute of Advanced Study (KIAS) which suppor\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for k,p in enumerate(p_lst):\n",
    "    print(k,preds[k],p[:100])\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let                      O \n",
      "_inline_math_            O \n",
      "be                       O \n",
      "a                        O \n",
      "nonzero                  O \n",
      "element                  O \n",
      "of                       O \n",
      "_inline_math_            O \n",
      ",                        O \n",
      "let                      O \n",
      "_inline_math_            O \n",
      "be                       O \n",
      "an                       O \n",
      "open              B-DFNDUM \n",
      "set               I-DFNDUM \n",
      "throughout               O \n",
      "which                    O \n",
      "_inline_math_            O \n",
      ",                        O \n",
      "and                      O \n",
      "let                      O \n",
      "_inline_math_            O \n",
      ".                        O \n",
      "Then                     O \n",
      "there                    O \n",
      "exists                   O \n",
      "a                        O \n",
      "neighbourhood            O \n",
      "_inline_math_            O \n",
      "of                       O \n",
      "_inline_math_            O \n",
      "such                     O \n",
      "that                     O \n",
      "for                      O \n",
      "all                      O \n",
      "_inline_math_            O \n",
      "_inline_math_            O \n",
      ".                        O \n"
     ]
    }
   ],
   "source": [
    "chunk = tree2conlltags(chunker.parse(pos_tag(word_tokenize(p_lst[63]))))\n",
    "for tok in chunk:\n",
    "    print('{:15} {:>10} '.format(tok[0], tok[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/chunker.pickle', 'wb') as chunker_f:\n",
    "    pickle.dump(chunker, chunker_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/vectorizer.pickle', 'wb') as token_f:\n",
    "    pickle.dump(, token_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
