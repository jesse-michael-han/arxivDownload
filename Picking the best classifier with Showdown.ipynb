{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def warn(*args, **kwargs): pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#train = pd.read_csv('../input/train.csv')\n",
    "#test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import sys\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "import nltk.data\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import pickle\n",
    "from collections import Iterable\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "    naive_bayes.MultinomialNB(),\n",
    "    KNeighborsClassifier(2),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    #GaussianNB(),\n",
    "    #LinearDiscriminantAnalysis(),\n",
    "    #QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "#Local imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from unwiki import unwiki\n",
    "import ner\n",
    "import parsing_xml as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Clean function to cleanse and standarize words\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalize\n",
    "\n",
    "#prepare the dataset\n",
    "allData = pd.DataFrame()\n",
    "with open('../sample18/defs.txt','r') as f1:\n",
    "    all_data_texts = f1.readlines()\n",
    "all_data_labels = len(all_data_texts)*[1.0]\n",
    "with open('../sample18/nondefs.txt', 'r') as f2:\n",
    "    all_data_texts_rand = f2.readlines()\n",
    "all_data_texts += all_data_texts_rand\n",
    "all_data_labels += len(all_data_texts_rand)*[0.0]\n",
    "\n",
    "# 1.0 will represent definitions is true 0.0 means it is false (not a definition)\n",
    "allData['labels'] = all_data_labels\n",
    "allData['texts'] = all_data_texts\n",
    "\n",
    "# Split and randomize the datasets\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(allData['texts'], allData['labels'])\n",
    "\n",
    "# Vectorize all the paragraphs and definitions in the dataset\n",
    "count_vect = CountVectorizer(analyzer='word', tokenizer=nltk.word_tokenize, ngram_range=(1,3))\n",
    "count_vect.fit(allData['texts'])\n",
    "xtrain = count_vect.transform(train_x)\n",
    "xtest = count_vect.transform(test_x)\n",
    "\n",
    "# Train Multinomial Naive Bayes model and print test metrics\n",
    "#clf = naive_bayes.MultinomialNB().fit(xtrain, train_y)\n",
    "#predictions = clf.predict(xtest)\n",
    "#print(metrics.classification_report(predictions,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_lst = [1550, 1650, 1750]\n",
    "#SVC_lst = [SVC(kernel=\"rbf\", C=param, probability=True) for param in param_lst]\n",
    "classifiers=  [ naive_bayes.MultinomialNB(),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "MultinomialNB ,  C= 0\n",
      "****Results****\n",
      "Accuracy: 86.1442%\n",
      "Log Loss: 2.561787368140119\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "#for C_param, clf in zip(param_lst,SVC_lst):\n",
    "for C_param, clf in enumerate(classifiers):\n",
    "    name = clf.__class__.__name__\n",
    "    clf.fit(xtrain, train_y)\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name, ',  C=', C_param)\n",
    "    \n",
    "    print('****Results****')\n",
    "    predictions = clf.predict(xtest)\n",
    "    acc = accuracy_score(test_y, predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    #prodictions = prob_prediction\n",
    "    prodictions = clf.predict_proba(xtest)\n",
    "    ll = log_loss(test_y, prodictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "    #log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    #log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.91      0.85      2358\n",
      "         1.0       0.95      0.88      0.91      4520\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6878\n",
      "   macro avg       0.87      0.89      0.88      6878\n",
      "weighted avg       0.90      0.89      0.89      6878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(predictions,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../PickleJar/tokenizer.pickle', 'wb') as class_f:\n",
    "    pickle.dump(word_tokenize, class_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
