{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def warn(*args, **kwargs): pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#train = pd.read_csv('../input/train.csv')\n",
    "#test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import gzip\n",
    "from lxml import etree\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "import nltk.data\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import pickle\n",
    "from collections import Iterable\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "    naive_bayes.MultinomialNB(),\n",
    "    KNeighborsClassifier(2),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    NuSVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    #GaussianNB(),\n",
    "    #LinearDiscriminantAnalysis(),\n",
    "    #QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "#Local imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from unwiki import unwiki\n",
    "import ner\n",
    "import parsing_xml as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition count: 123010.   NonDefinitions count: 119642. Total: 242652\n",
      "CPU times: user 3.13 s, sys: 313 ms, total: 3.45 s\n",
      "Wall time: 3.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xml_lst = glob.glob(\"/mnt/training_defs/math1*/*.xml.gz\")\n",
    "#allData = pd.DataFrame()\n",
    "all_data_texts = []\n",
    "all_data_labels = []\n",
    "def_cnt = 0\n",
    "nondef_cnt = 0\n",
    "for X in xml_lst[:-2000]:\n",
    "    tar_tree = etree.parse(X)\n",
    "    def_lst = tar_tree.findall('.//definition')\n",
    "    nondef_lst = tar_tree.findall('.//nondef')\n",
    "    all_data_texts += [D.text for D in def_lst]\n",
    "    all_data_labels += len(def_lst)*[1.0]\n",
    "    all_data_texts += [D.text for D in nondef_lst]\n",
    "    all_data_labels += len(nondef_lst)*[0.0]\n",
    "    def_cnt += len(def_lst)\n",
    "    nondef_cnt += len(nondef_lst)\n",
    "print(\"Definition count: %s.   NonDefinitions count: %s. Total: %s\"%(def_cnt, nondef_cnt, (def_cnt+nondef_cnt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20min 37s, sys: 5.38 s, total: 20min 42s\n",
      "Wall time: 20min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define Clean function to cleanse and standarize words\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalize\n",
    "\n",
    "#prepare the dataset Using 2018 old dataset\n",
    "#allData = pd.DataFrame()\n",
    "#with open('../sample18/defs.txt','r') as f1:\n",
    "#    all_data_texts = f1.readlines()\n",
    "#all_data_labels = len(all_data_texts)*[1.0]\n",
    "#with open('../sample18/nondefs.txt', 'r') as f2:\n",
    "#    all_data_texts_rand = f2.readlines()\n",
    "#all_data_texts += all_data_texts_rand\n",
    "#all_data_labels += len(all_data_texts_rand)*[0.0]\n",
    "\n",
    "# 1.0 will represent definitions is true 0.0 means it is false (not a definition)\n",
    "#allData['labels'] = all_data_labels\n",
    "#allData['texts'] = all_data_texts\n",
    "\n",
    "# Split and randomize the datasets\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(all_data_texts, all_data_labels)\n",
    "\n",
    "# Vectorize all the paragraphs and definitions in the dataset\n",
    "count_vect = CountVectorizer(analyzer='word', tokenizer=nltk.word_tokenize, ngram_range=(1,3))\n",
    "count_vect.fit(all_data_texts)\n",
    "xtrain = count_vect.transform(train_x)\n",
    "xtest = count_vect.transform(test_x)\n",
    "\n",
    "# Train Multinomial Naive Bayes model and print test metrics\n",
    "#clf = naive_bayes.MultinomialNB().fit(xtrain, train_y)\n",
    "#predictions = clf.predict(xtest)\n",
    "#print(metrics.classification_report(predictions,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 633 ms, sys: 19.6 ms, total: 653 ms\n",
      "Wall time: 3.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 1., 0., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "Def = ['a banach space is defined as a complete vector space.',\n",
    "       'This is not a definition honestly. even if it includes technical words like scheme and cohomology',\n",
    "      'There is no real reason as to why this classifier is so good.',\n",
    "      'a triangle is equilateral if and only if all its sides are the same length.',\n",
    "      ' The paper is organized as follows. ',\n",
    "      'Proof. By definition (6.4) _display_math_ where _inline_math_ denotes the parity of _inline_math_ and _display_math_',\n",
    "      'Counting subobjects over finite fields, as in Ringel _citation_.']\n",
    "vdef = count_vect.transform(Def)\n",
    "clf.predict(vdef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../PickleJar/count_vectorizer.pickle', 'rb') as pickle_obj:\n",
    "    count_vect = pickle.load(pickle_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_lst = [1550, 1650, 1750]\n",
    "#SVC_lst = [SVC(kernel=\"rbf\", C=param, probability=True) for param in param_lst]\n",
    "#classifiers=  [ naive_bayes.MultinomialNB(),]\n",
    "classifiers=  [SVC(kernel=\"rbf\", C=1600, probability=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-31-97796aa0bddd>\", line 4, in <module>\n",
      "    clf.fit(xtrain, train_y)\n",
      "  File \"/usr/lib/python3.8/site-packages/sklearn/svm/_base.py\", line 177, in fit\n",
      "    X_var = ((X.multiply(X)).mean() - (X.mean()) ** 2\n",
      "  File \"/usr/lib/python3.8/site-packages/scipy/sparse/base.py\", line 1082, in mean\n",
      "    return (inter_self / np.array(\n",
      "  File \"/usr/lib/python3.8/site-packages/scipy/sparse/base.py\", line 617, in __truediv__\n",
      "    return self._divide(other, true_divide=True)\n",
      "  File \"/usr/lib/python3.8/site-packages/scipy/sparse/base.py\", line 582, in _divide\n",
      "    return self.astype(np.float_)._mul_scalar(1./other)\n",
      "  File \"/usr/lib/python3.8/site-packages/scipy/sparse/data.py\", line 74, in astype\n",
      "    return self.copy()\n",
      "  File \"/usr/lib/python3.8/site-packages/scipy/sparse/data.py\", line 91, in copy\n",
      "    return self._with_data(self.data.copy(), copy=True)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 745, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "#for C_param, clf in zip(param_lst,SVC_lst):\n",
    "for C_param, clf in enumerate(classifiers):\n",
    "    name = clf.__class__.__name__\n",
    "    clf.fit(xtrain, train_y)\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name, ',  C=', C_param)\n",
    "    \n",
    "    print('****Results****')\n",
    "    predictions = clf.predict(xtest)\n",
    "    acc = accuracy_score(test_y, predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    #prodictions = prob_prediction\n",
    "    prodictions = clf.predict_proba(xtest)\n",
    "    ll = log_loss(test_y, prodictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "    \n",
    "    #log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    #log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.91      0.91     29558\n",
      "         1.0       0.92      0.91      0.91     31105\n",
      "\n",
      "    accuracy                           0.91     60663\n",
      "   macro avg       0.91      0.91      0.91     60663\n",
      "weighted avg       0.91      0.91      0.91     60663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(predictions,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_nondef = [D.text for D in nondef_lst[:15]]\n",
    "clf.predict(count_vect.transform(ex_nondef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_tree = etree.parse('/mnt/training_defs/math10/1002_005.xml.gz')\n",
    "def_lst = tar_tree.findall('.//definition')\n",
    "nondef_lst = tar_tree.findall('.//nondef')\n",
    "ex_text = [D.text for D in nondef_lst[:100]]\n",
    "sum(clf.predict(count_vect.transform(ex_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../PickleJar/count_vectorizer49.pickle', 'wb') as class_f:\n",
    "    pickle.dump(count_vect, class_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4939532"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the n-grams\n",
    "ww = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like the multivariate',\n",
       " 'the multivariate autoregressive',\n",
       " 'multivariate autoregressive process',\n",
       " 'autoregressive process that',\n",
       " '3. for each',\n",
       " 'sum _display_math_ throughout',\n",
       " 'absolute values less',\n",
       " 'stationary distribution which',\n",
       " 'normal with the',\n",
       " 'zero vector as',\n",
       " 'vector as mean',\n",
       " 'as mean and',\n",
       " 'the original univariate',\n",
       " 'original univariate process',\n",
       " 'univariate process _inline_math_',\n",
       " 'the element at',\n",
       " 'measures induced',\n",
       " 'probability measures induced',\n",
       " 'measures induced by',\n",
       " 'rate of speed',\n",
       " 'of speed and',\n",
       " 'speed and the',\n",
       " 'folner',\n",
       " '_inline_math_-maximality',\n",
       " 'some amenable',\n",
       " 'weak folner',\n",
       " 'folner sequence',\n",
       " 'an _inline_math_-maximality',\n",
       " '_inline_math_-maximality condition',\n",
       " 'be some amenable',\n",
       " 'some amenable group',\n",
       " 'group , along',\n",
       " 'a weak folner',\n",
       " 'weak folner sequence',\n",
       " 'folner sequence _inline_math_',\n",
       " 'be some bounded',\n",
       " 'bounded , additive',\n",
       " ', additive process',\n",
       " 'additive process as',\n",
       " 'satisfies an _inline_math_-maximality',\n",
       " 'an _inline_math_-maximality condition',\n",
       " '_inline_math_-maximality condition for',\n",
       " 'tiling-admissible',\n",
       " 'is tiling-admissible',\n",
       " 'tiling-admissible if',\n",
       " 'each _inline_math_-quasi',\n",
       " '_inline_math_-quasi tiling',\n",
       " 'tiling coming',\n",
       " 'uniform decomposition',\n",
       " 'decomposition tower',\n",
       " '_inline_math_ is tiling-admissible',\n",
       " 'is tiling-admissible if',\n",
       " 'tiling-admissible if there',\n",
       " 'finite sequence for',\n",
       " 'that each _inline_math_-quasi',\n",
       " 'each _inline_math_-quasi tiling',\n",
       " '_inline_math_-quasi tiling coming',\n",
       " 'tiling coming from',\n",
       " 'a uniform decomposition',\n",
       " 'uniform decomposition tower',\n",
       " 'decomposition tower (',\n",
       " 'tower ( cf',\n",
       " 'with basis sets',\n",
       " '_inline_math_-admissible with constant',\n",
       " 'find _display_math_ substituting',\n",
       " 'substituting ( 4.2',\n",
       " '4.2 ) in',\n",
       " '4.1 ) and',\n",
       " 'account ( 4.3',\n",
       " 'we check _display_math_',\n",
       " 'inequalities written',\n",
       " 'depending in',\n",
       " 'the inequalities written',\n",
       " 'inequalities written in',\n",
       " 'article _inline_math_ denotes',\n",
       " '( depending in',\n",
       " 'depending in general',\n",
       " 'general only on',\n",
       " 'controls which',\n",
       " 'exactly assumption',\n",
       " 'lemmas that imply',\n",
       " 'imply the null',\n",
       " ') with controls',\n",
       " 'with controls which',\n",
       " 'controls which are',\n",
       " 'derivatives of smooth',\n",
       " 'of smooth enough',\n",
       " 'enough functions having',\n",
       " 'a small support',\n",
       " 'that the controls',\n",
       " 'controls are in',\n",
       " 'is exactly assumption',\n",
       " 'exactly assumption _inline_math_',\n",
       " 'account remark .',\n",
       " ') that brings',\n",
       " 'that brings the',\n",
       " 'brings the initial',\n",
       " 'then prove in',\n",
       " 'some appropriate weighted',\n",
       " 'space ( proposition']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the n-grams (same as features) with identity number (not freq count)\n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fibre _inline_math_ and',\n",
       " 'fibre _inline_math_ are',\n",
       " 'fibre _inline_math_ as',\n",
       " 'fibre _inline_math_ associated',\n",
       " 'fibre _inline_math_ au',\n",
       " 'fibre _inline_math_ belongs',\n",
       " 'fibre _inline_math_ can',\n",
       " 'fibre _inline_math_ denotes',\n",
       " 'fibre _inline_math_ for',\n",
       " 'fibre _inline_math_ has',\n",
       " 'fibre _inline_math_ have',\n",
       " 'fibre _inline_math_ hence',\n",
       " 'fibre _inline_math_ in',\n",
       " 'fibre _inline_math_ intersects',\n",
       " 'fibre _inline_math_ into',\n",
       " 'fibre _inline_math_ is',\n",
       " 'fibre _inline_math_ itself',\n",
       " 'fibre _inline_math_ mais',\n",
       " 'fibre _inline_math_ modulo',\n",
       " 'fibre _inline_math_ must',\n",
       " 'fibre _inline_math_ notice',\n",
       " 'fibre _inline_math_ of',\n",
       " 'fibre _inline_math_ on',\n",
       " 'fibre _inline_math_ over',\n",
       " 'fibre _inline_math_ où',\n",
       " 'fibre _inline_math_ pour',\n",
       " 'fibre _inline_math_ smooth',\n",
       " 'fibre _inline_math_ smoothly',\n",
       " 'fibre _inline_math_ such',\n",
       " 'fibre _inline_math_ then',\n",
       " 'fibre _inline_math_ where',\n",
       " 'fibre _inline_math_ which',\n",
       " 'fibre _inline_math_ with',\n",
       " 'fibre _inline_math_-plectically',\n",
       " 'fibre _inline_math_-plectically if',\n",
       " 'fibre _inline_math_.this',\n",
       " 'fibre _inline_math_.this cocycle',\n",
       " 'fibre a',\n",
       " 'fibre a punctured',\n",
       " 'fibre a smooth',\n",
       " 'fibre a truncated',\n",
       " 'fibre an',\n",
       " 'fibre an affine',\n",
       " 'fibre and',\n",
       " 'fibre and a',\n",
       " 'fibre and bundle',\n",
       " 'fibre and zero',\n",
       " 'fibre are',\n",
       " 'fibre are geometrically',\n",
       " 'fibre as',\n",
       " 'fibre as _inline_math_',\n",
       " 'fibre as well',\n",
       " 'fibre associated',\n",
       " 'fibre associated to',\n",
       " 'fibre at',\n",
       " 'fibre at _inline_math_',\n",
       " 'fibre at a',\n",
       " 'fibre at the',\n",
       " 'fibre au-dessus',\n",
       " 'fibre au-dessus de',\n",
       " 'fibre bundle',\n",
       " 'fibre bundle )',\n",
       " 'fibre bundle ,',\n",
       " 'fibre bundle .',\n",
       " 'fibre bundle :',\n",
       " 'fibre bundle _inline_math_',\n",
       " 'fibre bundle and',\n",
       " 'fibre bundle approach',\n",
       " 'fibre bundle associated',\n",
       " 'fibre bundle carrying',\n",
       " 'fibre bundle decomposition',\n",
       " 'fibre bundle in',\n",
       " 'fibre bundle is',\n",
       " 'fibre bundle map',\n",
       " 'fibre bundle means',\n",
       " 'fibre bundle of',\n",
       " 'fibre bundle on',\n",
       " 'fibre bundle over',\n",
       " 'fibre bundle satisfying',\n",
       " 'fibre bundle universal',\n",
       " 'fibre bundle was',\n",
       " 'fibre bundle which',\n",
       " 'fibre bundle with',\n",
       " 'fibre bundle yielding',\n",
       " 'fibre bundles',\n",
       " 'fibre bundles )',\n",
       " 'fibre bundles ,',\n",
       " 'fibre bundles .',\n",
       " 'fibre bundles _display_math_',\n",
       " 'fibre bundles _inline_math_',\n",
       " 'fibre bundles are',\n",
       " 'fibre bundles having',\n",
       " 'fibre bundles if',\n",
       " 'fibre bundles over',\n",
       " 'fibre bundles we',\n",
       " 'fibre bundles with',\n",
       " 'fibre by',\n",
       " 'fibre by _inline_math_',\n",
       " 'fibre c_inline_math_-algebra',\n",
       " 'fibre c_inline_math_-algebra _inline_math_']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 2000000\n",
    "l = 100\n",
    "ww[s:(s+l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
